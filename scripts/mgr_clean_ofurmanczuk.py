# -*- coding: utf-8 -*-
"""mgr_clean_OFurmanczuk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UvrR5_pR9oIYu3GKC6IW9kWYUmTsPQ55
"""


!pip install shap
from io import StringIO
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import *
from sklearn.model_selection import GridSearchCV
import shap
import time
import matplotlib.ticker as mtick
from matplotlib.pyplot import figure
import datetime

from google.colab import drive

drive.mount("/content/gdrive", force_remount=True)

"""--- ZBIOR CDC ---"""

df = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/COVID_Cases_Restricted_Detailed_02282021.csv")

# Wybranie wylacznie laboratoryjnie potwierdzonych przypadkow
df = df[df["current_status"] == "Laboratory-confirmed case"]

# Usuniecie nieistotnych i mniej istotnych zmiennych
df = df.drop(["cdc_report_dt", "onset_dt", "pos_spec_dt", "current_status", "res_county"],
             axis=1)

# ze wzgledu na bardzo duzy wolumen danych do eksploracji danych posluzono sie losowowo
# wybranym 1% danych
df_sample = df.sample(frac=0.01)
df.shape

"""Przeglad danych uzyskanych od CDC"""

# Rozmiar zbioru CDC
df.shape

# Wszystkie zmienne z wejsciowego zbioru CDC
df.columns

# Typ zmiennych w zbiorze CDC
df.info()

"""Przeglad zmiennych pod katem brakujacych danych w zbiorze CDC"""

l = []
l_plot = []
df_t1 = df_sample[(df_sample['death_yn'] != 'Missing') &
                  (df_sample['death_yn'] != 'Unknown')]
for col in df_t1.columns:
    nan = round(sum(df_t1[col].isna()) / df_t1.shape[0], 3)
    unknown = round(sum(df_t1[col] == "Unknown") / df_t1.shape[0], 3)
    missing = round(sum(df_t1[col] == "Missing") / df_t1.shape[0], 3)
    l.append({col: {"nan": nan, "unknown": unknown, "missing": missing}})
    l_plot.append([nan, unknown, missing])

figure(num=None, figsize=(40, 10), dpi=80, facecolor='w', edgecolor='k')

bars1 = [x[0] for x in l_plot]
bars2 = [x[1] for x in l_plot]
bars3 = [x[2] for x in l_plot]

barWidth = 0.25

r1 = np.arange(len(bars1))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]

plt.autoscale()
plt.bar(r1, bars1, color='blue', width=barWidth,
        edgecolor='white', label='NaN')
plt.bar(r2, bars2, color='red', width=barWidth,
        edgecolor='white', label='Missing')
plt.bar(r3, bars3, color='green', width=barWidth,
        edgecolor='white', label='Unknown')

plt.xticks([r + barWidth for r in range(len(bars1))], df.columns)

plt.legend()
plt.show()

"""Sprawdzenie skumulowanego rozkladu ilosci brakujacych danych w poszczegolnych zmiennych"""

df_recoded_NA = df_sample.replace(to_replace=["Unknown", "Missing"],
                                  value=np.nan)

for col in df_recoded_NA.columns:
    try:
        df_recoded_NA.loc[:, col] = pd.to_numeric(df_recoded_NA[col])
    except:
        pass

df_without_NA = df_recoded_NA.dropna()
non_NaN = [df_recoded_NA.dropna(thresh=i).shape[0] / df.shape[0]
           for i in range(len(df.columns) + 1)]

plt.plot(non_NaN)

"""Usuniecie obserwacji zawierajacych brakujace dane"""

df = df.dropna()
df = df.replace(to_replace="Unknown", value=np.nan)
df = df.dropna()
df = df.replace(to_replace="Missing", value=np.nan)
df = df.dropna()

# df.to_csv("/content/gdrive/MyDrive/dane-mgr" +
# "/clean_II_COVID_Cases_Restricted_Detailed_02282021.csv",
#   index=False)

"""Przeglad zbioru CDC z pelnymi obserwacjami"""

df = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/clean_II_COVID_Cases_Restricted_Detailed_02282021.csv")

# Usuniecie obserwacji pochodzacych spoza kontynentalnej czesci USA
df = df.drop(df[df.res_state == "VI"].index)

# Czestotliwosc wystepowania
freq_df = df.groupby('race_ethnicity_combined').sex.value_counts().unstack()
freq_df.plot(kind="barh", stacked=False, grid=True)

# Udzial bialych pacjentow w zbiorze
len(df[df.race_ethnicity_combined == "White, Non-Hispanic"]) / len(df)

# Udzial mezczyzn w zbiorze
len(df[df.sex == "Male"]) / len(df)

# Stworzenie zmiennej date_month

dates = df.cdc_case_earliest_dt.values

# df["date_day"] = list(map(lambda x: int(x.split("-")[2]), dates))
# df["date_year"] = list(map(lambda x: int(x.split("-")[0]), dates))
df["date_month"] = list(map(lambda x: int(x.split("-")[1]), dates))

df.head()

# Przekodowanie zmiennych: No -> 0; Yes -> 1; Male -> 1; Female -> 0

df = df.replace("No", 0).replace("Yes", 1)
df = df.replace("Male", 1).replace("Female", 0)

# Czestosc wystepowania poszczegolnych objawow

df[['fever_yn', 'sfever_yn',
    'chills_yn', 'myalgia_yn', 'runnose_yn', 'sthroat_yn', 'cough_yn',
    'sob_yn', 'nauseavomit_yn', 'headache_yn', 'abdom_yn', 'diarrhea_yn',
    'medcond_yn']].sum() / len(df) * 100

# Wykres czestosci wystepowania poszczegolnych objawow
binary_freq = df[['fever_yn', 'sfever_yn',
                  'chills_yn', 'myalgia_yn', 'runnose_yn', 'sthroat_yn', 'cough_yn',
                  'sob_yn', 'nauseavomit_yn', 'headache_yn', 'abdom_yn', 'diarrhea_yn',
                  'medcond_yn']].sum() / len(df) * 100

ax = binary_freq.sort_values(ascending=False) \
    .plot(kind="barh", grid=True, figsize=(9, 5))

ax.xaxis.set_major_formatter(mtick.PercentFormatter())

# Smiertelnosc COVID-19 w USA

df[df["death_yn"] == 1].shape[0] / (df[df["death_yn"] == 0] \
                                    .shape[0] + df[df["death_yn"] == 1].shape[0])

"""---ZBIORY DANYCH METEOROLOGICZNYCH I DANYCH DEMOGRAFICZNYCH---"""

# Zrodlo: https://www.ncdc.noaa.gov/ghcn/comparative-climatic-data

df_humidity = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/climate/clean-average-humidity-US.csv", sep=";")
df_temp = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/climate/clean-average-temp-US.csv", sep=";")
df_wind = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/climate/clean-average-wind-US.csv", sep=";")

# Zrodlo:
# https://hub.arcgis.com/datasets/48f9af87daa241c4b267c5931ad3b226_0/

df_demog = pd.read_csv("/content/gdrive/MyDrive/dane-mgr/USA_Counties.csv", index_col="FIPS")

# Storzenie DF ze wszystkimi zmiennymi meteorologicznymi
df_humidity_state = df_humidity.groupby("STATE").mean().round(2)
df_temp_state = df_temp.groupby("STATE").mean().round(2)
df_wind_state = df_wind.groupby("STATE").mean().round(2)

df_weather = df_humidity_state.join(df_temp_state) \
    .rename(columns={
    'ANNUAL-A': 'humidity_afternoon', 'ANNUAL-M': 'humidity_morning', 'ANN': 'temperature'},
            inplace=False) \
    .join(df_wind_state)[['humidity_afternoon', 'humidity_morning', 'temperature', 'ANN']] \
    .rename(columns={'ANN': 'wind_speed'}, inplace=False)[
    ['humidity_afternoon', 'humidity_morning', 'temperature', 'wind_speed']] \

# Usuniecie stanow z brakujacymi danymi oraz stanow spoza kontynentalnej czesci USA
df_weather = df_weather.drop(["DC", "NH", "PC", "PR", "HI"])

# Wykres srednich ze zmiennych meteorologicznych dla poszczegolnych stanow
fig, axes = plt.subplots(3)

fig.set_figheight(13)
fig.set_figwidth(17)

df_weather["temperature"].plot(ax=axes[0], kind="bar", legend=False, xlabel="",
                               title="Temperatura", ylabel="[?F]", rot=0)
df_weather["wind_speed"].plot(ax=axes[1], kind="bar", legend=False, xlabel="",
                              title="Sila wiatru", ylabel="[MPH]", rot=0)
df_weather[["humidity_afternoon", "humidity_morning"]].plot(ax=axes[2], xlabel="",
                             kind="bar", title="Wilgotnosc powietrza", ylabel="[%]", rot=0)

axes[0].set_ylim([30, 75])
axes[1].set_ylim([5, 12])
axes[2].set_ylim([20, 90])

# Usuniecie obserwacji z brakujaca wartoscia liczebnosci populacji
df_us_filled = df_demog[df_demog["POPULATION"] > 0]

# Srednie zageszczenie ludnosci w USA
df_us_filled["POPULATION"].sum() / df_us_filled["SQMI"].sum()

# Powierzchnia calkowita USA
df_us_filled["SQMI"].sum()

# Storzenie ostatecznego zbioru danych
res_states = df.res_state.values
date_months = df.date_month.values
fips_codes = df.county_fips_code.values

avg_temp = []
avg_wind = []
avg_humidity_A = []
avg_humidity_M = []
pop_density = []

for i in range(len(res_states)):
    state = res_states[i]
    month = date_months[i]
    fips = int(fips_codes[i])

    temp = df_temp_state[str(month) + ".00"][state]
    wind = df_wind_state[str(month)][state]
    humidity_morning = df_humidity_state[str(month) + "-M"][state]
    humidity_afternoon = df_humidity_state[str(month) + "-A"][state]
    density = df_demog["POP_SQMI"][fips]

    avg_temp.append(temp)
    avg_wind.append(wind)
    avg_humidity_A.append(humidity_afternoon)
    avg_humidity_M.append(humidity_morning)
    pop_density.append(density)

df["avg_temp"] = avg_temp
df["avg_wind"] = avg_wind
df["avg_humidity_A"] = avg_humidity_A
df["avg_humidity_M"] = avg_humidity_M
df["pop_density"] = pop_density

# df.to_csv("/content/gdrive/MyDrive/dane-mgr
# /clean_II_more_data-COVID_Cases_Restricted_Detailed_02282021.csv", index=False)

"""--- STWORZENIE MODELU ---"""

df = pd.read_csv(
    "/content/gdrive/MyDrive/dane-mgr/" +
    "clean_II_more_data-COVID_Cases_Restricted_Detailed_02282021.csv")

# Usuniecie zmiennych, ktore nie beda przydatne na etapie tworzenia modelu
# df = df.drop(["county_fips_code", "res_county",
#               "res_state", "cdc_case_earliest_dt",
#               "date_month", "current_status"], axis=1)

df = df.drop(["county_fips_code",
              "res_state", "cdc_case_earliest_dt",
              "date_month"], axis=1)

df_copy = df.copy()

# Rozmiar ostatecznego zbioru danych
df.shape

# Zmienne, ktore wziely udzial w budowie modelu
df.columns

# Transformacja zmiennych na numeryczne
for col in df.columns:
    try:
        df.loc[:, col] = pd.to_numeric(df[col])
    except:
        pass

# Storzenie wektora zmiennej objasnianej oraz macierzy zmiennych objasniajacych
y = df.death_yn
X = df.drop("death_yn", axis=1)

# One hot encoding zmiennych jakosciowych
X = pd.get_dummies(X)

# Rozdzielenie zbioru na trenigowy, walidacyjny i testowy
# 0.2 - testowy, 0.2 - validacyjny, 0.6 - treningowy

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=31)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=31)  # 0.25 x 0.8 = 0.2

y_test.sum()


# Funkcja umozliwiajaca wybranie najlepszej kombinacji hiperparametrow

def find_best_params(X_train, y_train, X_val, y_val, params):
    def check_parameters(learning_rate,
                         max_depth, min_child_weight,
                         scale_pos_weight, gamma=0):
        model = XGBClassifier(
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_child_weight=min_child_weight,
            gamma=gamma,
            objective='binary:logistic',
            use_label_encoder=False,
            scale_pos_weight=scale_pos_weight
        )
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        acc = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)

        return acc, f1

    best_f1 = 0
    best_param = {
        "learning_rate": -1,
        "max_depth": -1,
        "min_child_weight": -1,
        "gamma": -1,
        "scale_pos_weight": -1
    }

    for learning_rate in params["learning_rate"]:
        for max_depth in params["max_depth"]:
            for min_child_weight in params["min_child_weight"]:
                for gamma in params["gamma"]:
                    for scale_pos_weight in params["scale_pos_weight"]:

                        start = time.time()
                        acc, f1 = check_parameters(learning_rate=learning_rate,
                                                   max_depth=max_depth,
                                                   min_child_weight=min_child_weight,
                                                   scale_pos_weight=scale_pos_weight,
                                                   gamma=gamma)
                        end = time.time()
                        # print(end - start)
                        # print("\nDokladnosc: {:.4f}; F1-score: {:.4f}".format(acc, f1))

                        if (best_f1 < f1):
                            # print("NEW BEST")
                            best_param = {
                                "learning_rate": learning_rate,
                                "max_depth": max_depth,
                                "min_child_weight": min_child_weight,
                                "gamma": gamma,
                                "scale_pos_weight": scale_pos_weight
                            }
                            print("\nDokladnosc: {:.4f}; F1-score: {:.4f}".format(acc, f1))
                            print(best_param)

                            best_f1 = f1
    return best_param


# Propozycje hiperparametrow

parameters = {
    "learning_rate": [0.05, 0.10, 0.15],
    "max_depth": [4, 6, 8],
    "min_child_weight": [3, 5, 7],
    "scale_pos_weight": [3, 5, 10],
    "gamma": [0.0, 0.1]

}

# parameters = {"learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
#  "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
#  "min_child_weight" : [ 1, 3, 5, 7 ],
#  "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
#  "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ] }

# Wywowlanie funkcji find_best_params
print("--- czas rozpoczecia szukania optymalnych hiperparametrow: {0} ---"
      .format(datetime.datetime.now()))

best_params = find_best_params(X_train, y_train, X_val, y_val, parameters)

print("--- czas ukonczenia szukania optymalnych hiperparametrow: {0} ---"
      .format(datetime.datetime.now()))

print(best_params)

best_params = {'learning_rate': 0.05, 'max_depth': 4,
               'min_child_weight': 5, 'gamma': 0.0, 'scale_pos_weight': 5}

"""Najlepsze parametry:
{'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 5, 
'gamma': 0.0, 'scale_pos_weight': 5}
"""

# Storzenie modelu dla najlepszych hiperparametrow
best_model = XGBClassifier(
    learning_rate=best_params["learning_rate"],
    max_depth=best_params["max_depth"],
    min_child_weight=best_params["min_child_weight"],
    gamma=best_params["gamma"],
    objective="binary:logistic",
    use_label_encoder=False,
    scale_pos_weight=best_params["scale_pos_weight"]
)
best_model.fit(X_train, y_train)

# Obliczenie dokladnosci i F1-score na danych testowych
y_pred = best_model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nDokladnosc: {:.4f}; F1-score: {:.4f}".format(acc, f1))

# Wykres krzywej ROC dla uzyskanego modelu
probs = best_model.predict_proba(X_test)
preds = probs[:, 1]
fpr, tpr, threshold = roc_curve(y_test, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# Macierz konfuzji dla storzonego modelu
plot_confusion_matrix(best_model, X_test, y_test, values_format=".0f",
                      labels=[1, 0], display_labels=["Zgon", "Ozdrowienie"],
                      cmap="Blues")

# Stworzenie zbalansowanego zbioru testowego
df_test = X_test.copy()
df_test["death_yn"] = y_test

df_test_y = df_test[df_test.death_yn == 1]
df_test_n = df_test[df_test.death_yn == 0].sample(df_test_y.shape[0], random_state=31)

df_test = df_test_y.append(df_test_n)

_y = df_test.death_yn
_X = df_test.drop("death_yn", axis=1)

# Obliczenie dokladnosci i F1-score na danych testowych - zbalansowany zbior testowy
y_pred = best_model.predict(_X)

acc = accuracy_score(_y, y_pred)
f1 = f1_score(_y, y_pred)

print("\nDokladnosc: {:.4f}; F1-score: {:.4f}".format(acc, f1))

# Macierz konfuzji dla storzonego modelu - zbalansowany zbior testowy
plot_confusion_matrix(best_model, _X, _y, values_format=".0f",
                      labels=[1, 0], display_labels=["Zgon", "Ozdrowienie"],
                      cmap="Blues")

# Wykres krzywej ROC dla uzyskanego modelu - zbalansowany zbior testowy
probs = best_model.predict_proba(_X)
preds = probs[:, 1]
fpr, tpr, threshold = roc_curve(_y, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""--- INTERPRETACJA MODELU ---"""

explainer = shap.TreeExplainer(best_model)

shap_values = explainer.shap_values(X_test)

shap_values.shape

explainer.expected_value

shap.summary_plot(shap_values, features=X_test,
                  feature_names=X_test.columns, plot_type="bar")

m = {}

for i, col in enumerate(X_test.columns):
    mean_abs = abs(shap_values[:, i]).mean()
    m[col] = mean_abs

sort_m = sorted(m.items(), key=lambda x: x[1], reverse=True)

for el in sort_m:
    print("{}: Srednia z bezwzglednych wartosci SHAP= {:.4f}".format(el[0], el[1]))

abs(shap_values[:, list(X_test.columns).index("hosp_yn")]).mean()

shap.initjs()
shap.dependence_plot("avg_temp", shap_values, X_test)

shap_values_XGB_test = explainer.shap_values(X_test)
shap.summary_plot(shap_values_XGB_test, X_test)